{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83998dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444fdb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_car_details(entry):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    link = entry[\"url\"]\n",
    "    car_data = entry.copy()\n",
    "\n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        container = soup.select_one(\"div.row.pt-3\")\n",
    "        if container:\n",
    "            details = container.select(\"div.flex.items-center\")\n",
    "            for detail in details:\n",
    "                text = detail.get_text(separator=\" \", strip=True)\n",
    "                if \":\" in text:\n",
    "                    key, value = text.split(\":\", 1)\n",
    "                    car_data[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
    "                elif \"VIN\" in text:\n",
    "                    car_data['vin'] = text.split(\"VIN:\")[1].strip()\n",
    "                elif \"Stock Number\" in text:\n",
    "                    car_data['stock_number'] = text.split(\"Stock Number:\")[1].strip()\n",
    "                elif \"miles\" in text:\n",
    "                    car_data['mileage'] = text.strip()\n",
    "                elif \"Listed\" in text:\n",
    "                    car_data['listed_since'] = text.strip()\n",
    "\n",
    "        options_container = soup.find('h2', string=\"Options & packages\")\n",
    "        if options_container:\n",
    "            options_list = [\n",
    "                item.get_text(separator=\" \", strip=True)\n",
    "                for item in options_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['options_and_packages'] = \", \".join(options_list)\n",
    "\n",
    "        popular_container = soup.find('h2', string=\"Popular features\")\n",
    "        if popular_container:\n",
    "            features_list = [\n",
    "                item.get_text(separator=\" \", strip=True)\n",
    "                for item in popular_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['popular_features'] = \", \".join(features_list)\n",
    "\n",
    "        standard_container = soup.find('h2', string=\"Standard features\")\n",
    "        if standard_container:\n",
    "            std_features_list = [\n",
    "                item.get_text(separator=\" \", strip=True)\n",
    "                for item in standard_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['standard_features'] = \", \".join(std_features_list)\n",
    "\n",
    "        price_section = soup.find('div', {'id': 'usedPriceGraph'})\n",
    "        if price_section:\n",
    "            line_items = price_section.select('div[data-test=\"usedListingPriceGraphLineItem\"]')\n",
    "            for item in line_items:\n",
    "                label = item.get(\"data-test-item\")\n",
    "                text = item.get_text(separator=\"|\", strip=True)\n",
    "                if label and \"|\" in text:\n",
    "                    _, value = text.split(\"|\")\n",
    "                    car_data[label.lower().replace(\" \", \"_\")] = value.strip()\n",
    "\n",
    "            quality_bars = price_section.select('div[data-test=\"priceRangeIconAndRange\"]')\n",
    "            for bar in quality_bars:\n",
    "                quality = bar.get(\"data-test-item\")\n",
    "                range_tag = bar.find(\"p\")\n",
    "                if quality and range_tag:\n",
    "                    car_data[f\"price_range_{quality.lower()}\"] = range_tag.text.strip()\n",
    "\n",
    "            description = price_section.find('div', {'data-test': 'usedListingPriceGraphDescription'})\n",
    "            if description:\n",
    "                car_data[\"price_description\"] = description.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        seller_notes_header = soup.find('h2', string=\"Seller Notes\")\n",
    "        if seller_notes_header:\n",
    "            seller_div = seller_notes_header.find_next('div', class_='see-more')\n",
    "            if seller_div:\n",
    "                car_data['seller_notes'] = seller_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting from {link}: {e}\")\n",
    "\n",
    "    return car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_all_car_details(json_path, output_json, output_csv):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        car_links = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    for entry in tqdm(car_links, desc=\"Scraping car details\"):\n",
    "        car_info = extract_car_details(entry)\n",
    "        results.append(car_info)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CLEANING: title → year, make, model, trim\n",
    "    def split_title(title):\n",
    "        if not isinstance(title, str): return [None]*4\n",
    "        parts = title.replace(\"Used\", \"\").strip().split(\" \")\n",
    "        year = parts[0] if len(parts) > 0 else None\n",
    "        make = parts[1] if len(parts) > 1 else None\n",
    "        model = parts[2] if len(parts) > 2 else None\n",
    "        trim = \" \".join(parts[3:]) if len(parts) > 3 else None\n",
    "        return [year, make, model, trim]\n",
    "\n",
    "    df[['year', 'make', 'model', 'trim']] = df['title'].apply(lambda x: pd.Series(split_title(x)))\n",
    "\n",
    "    # CLEANING: dealer_info → city, state, distance\n",
    "    if 'dealer_info' in df.columns:\n",
    "        df[['city_state', 'distance']] = df['dealer_info'].str.extract(r'(.+?,\\s*[A-Z]{2})\\s*\\((.*?)\\)', expand=True)\n",
    "        df[['city', 'state']] = df['city_state'].str.extract(r'(.+),\\s*([A-Z]{2})')\n",
    "\n",
    "    # CLEANING: price_description spacing\n",
    "    df['price_description'] = df['price_description'].str.replace(r'([a-zA-Z])(\\$)', r'\\1 \\2', regex=True)\n",
    "\n",
    "    # CLEANING: better sentence spacing in seller_notes\n",
    "    df['seller_notes'] = df['seller_notes'].str.replace(r'\\.(\\w)', r'. \\1', regex=True)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Cleaned and saved car details to:\\n- {output_csv}\\n- {output_json}\")\n",
    "\n",
    "\n",
    "def scrape_city_state(city, state, max_pages=20):\n",
    "    city_slug = city.lower().replace(\" \", \"-\")\n",
    "    state_slug = state.lower()\n",
    "    location_key = f\"{city_slug}_{state_slug}\"\n",
    "\n",
    "    all_links = []\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(1, max_pages + 1), desc=f\"{location_key} - Page\"):\n",
    "        url = f\"https://www.truecar.com/used-cars-for-sale/listings/location-{city_slug}-{state_slug}/?page={i}\"\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        link_tags = soup.select('a[data-test=\"cardLinkCover\"]')\n",
    "\n",
    "        for tag in link_tags:\n",
    "            relative_link = tag.get(\"href\")\n",
    "            full_link = \"https://www.truecar.com\" + relative_link if relative_link else None\n",
    "            if full_link:\n",
    "                card = tag.find_parent(\"div\", class_=\"card\")\n",
    "                car = {\"url\": full_link}\n",
    "                try:\n",
    "                    title_elem = card.select_one('[data-test=\"vehicleCardInfo\"]')\n",
    "                    if title_elem:\n",
    "                        car[\"title\"] = title_elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "                    mileage_elem = card.select_one('[data-test=\"vehicleMileage\"]')\n",
    "                    if mileage_elem:\n",
    "                        car[\"mileage_listed\"] = mileage_elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "                    price_elem = card.select_one('[data-test=\"vehicleCardPricingPrice\"]')\n",
    "                    if price_elem:\n",
    "                        car[\"list_price_displayed\"] = price_elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "                    dealer_elem = card.select_one('[data-test=\"vehicleCardFooter\"]')\n",
    "                    if dealer_elem:\n",
    "                        car[\"dealer_info\"] = dealer_elem.get_text(separator=\" \", strip=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error extracting card metadata: {e}\")\n",
    "\n",
    "                all_links.append(car)\n",
    "\n",
    "    json_path = f\"./data/truecar_links_{location_key}.json\"\n",
    "    csv_path = f\"./data/truecar_links_{location_key}.csv\"\n",
    "    output_json = f\"./data/truecar_details_{location_key}.json\"\n",
    "    output_csv = f\"./data/truecar_details_{location_key}.csv\"\n",
    "\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(all_links, f, indent=2)\n",
    "    pd.DataFrame(all_links).to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Saved {len(all_links)} links for {city.title()}, {state.upper()}\")\n",
    "    scrape_all_car_details(json_path=json_path, output_json=output_json, output_csv=output_csv)\n",
    "\n",
    "\n",
    "# You can use this block in your script to trigger the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    cities = [\n",
    "        (\"Boston\", \"MA\"),\n",
    "        (\"Austin\", \"TX\"),\n",
    "        (\"San Francisco\", \"CA\"),\n",
    "        (\"Seattle\", \"WA\"),\n",
    "        (\"Atlanta\", \"GA\"),\n",
    "        (\"Chicago\", \"IL\"),\n",
    "        (\"Detroit\", \"MI\")\n",
    "    ]\n",
    "    for city, state in cities:\n",
    "        scrape_city_state(city, state, max_pages=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e166136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
