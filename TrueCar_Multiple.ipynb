{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83998dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444fdb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_car_details(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    car_data = {'url': link}\n",
    "\n",
    "    try:\n",
    "        container = soup.select_one(\"div.row.pt-3\")\n",
    "        if container:\n",
    "            details = container.select(\"div.flex.items-center\")\n",
    "            for detail in details:\n",
    "                text = detail.get_text(separator=\" \", strip=True)\n",
    "                if \":\" in text:\n",
    "                    key, value = text.split(\":\", 1)\n",
    "                    car_data[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
    "                elif \"VIN\" in text:\n",
    "                    car_data['vin'] = text.split(\"VIN:\")[1].strip()\n",
    "                elif \"Stock Number\" in text:\n",
    "                    car_data['stock_number'] = text.split(\"Stock Number:\")[1].strip()\n",
    "                elif \"miles\" in text:\n",
    "                    car_data['mileage'] = text.strip()\n",
    "                elif \"Listed\" in text:\n",
    "                    car_data['listed_since'] = text.strip()\n",
    "\n",
    "        options_container = soup.find('h2', string=\"Options & packages\")\n",
    "        if options_container:\n",
    "            options_list = [\n",
    "                item.get_text(strip=True)\n",
    "                for item in options_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['options_and_packages'] = \", \".join(options_list)\n",
    "\n",
    "        popular_container = soup.find('h2', string=\"Popular features\")\n",
    "        if popular_container:\n",
    "            features_list = [\n",
    "                item.get_text(strip=True)\n",
    "                for item in popular_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['popular_features'] = \", \".join(features_list)\n",
    "\n",
    "        standard_container = soup.find('h2', string=\"Standard features\")\n",
    "        if standard_container:\n",
    "            std_features_list = [\n",
    "                item.get_text(strip=True)\n",
    "                for item in standard_container.find_next(\"div\").find_all(\"div\", class_=\"flex items-center\")\n",
    "            ]\n",
    "            car_data['standard_features'] = \", \".join(std_features_list)\n",
    "\n",
    "        price_section = soup.find('div', {'id': 'usedPriceGraph'})\n",
    "        if price_section:\n",
    "            line_items = price_section.select('div[data-test=\"usedListingPriceGraphLineItem\"]')\n",
    "            for item in line_items:\n",
    "                label = item.get(\"data-test-item\")\n",
    "                text = item.get_text(separator=\"|\", strip=True)\n",
    "                if label and \"|\" in text:\n",
    "                    _, value = text.split(\"|\")\n",
    "                    car_data[label.lower().replace(\" \", \"_\")] = value.strip()\n",
    "\n",
    "            quality_bars = price_section.select('div[data-test=\"priceRangeIconAndRange\"]')\n",
    "            for bar in quality_bars:\n",
    "                quality = bar.get(\"data-test-item\")\n",
    "                range_tag = bar.find(\"p\")\n",
    "                if quality and range_tag:\n",
    "                    car_data[f\"price_range_{quality.lower()}\"] = range_tag.text.strip()\n",
    "\n",
    "            description = price_section.find('div', {'data-test': 'usedListingPriceGraphDescription'})\n",
    "            if description:\n",
    "                car_data[\"price_description\"] = description.get_text(strip=True)\n",
    "\n",
    "        seller_notes_header = soup.find('h2', string=\"Seller Notes\")\n",
    "        if seller_notes_header:\n",
    "            seller_div = seller_notes_header.find_next('div', class_='see-more')\n",
    "            if seller_div:\n",
    "                car_data['seller_notes'] = seller_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting from {link}: {e}\")\n",
    "\n",
    "    return car_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fb7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_car_details(json_path, output_json, output_csv):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        car_links = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    for entry in tqdm(car_links, desc=\"Scraping car details\"):\n",
    "        link = entry[\"url\"]\n",
    "        car_info = extract_car_details(link)\n",
    "        results.append(car_info)\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Saved car details to:\\n- {output_json}\\n- {output_csv}\")\n",
    "\n",
    "\n",
    "def scrape_city_state(city, state, max_pages=2):\n",
    "    city_slug = city.lower().replace(\" \", \"-\")\n",
    "    state_slug = state.lower()\n",
    "    location_key = f\"{city_slug}_{state_slug}\"\n",
    "\n",
    "    all_links = []\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(1, max_pages + 1), desc=f\"{location_key} - Page\"):\n",
    "        url = f\"https://www.truecar.com/used-cars-for-sale/listings/location-{city_slug}-{state_slug}/?page={i}\"\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        link_tags = soup.select('a[data-test=\"cardLinkCover\"]')\n",
    "\n",
    "        for tag in link_tags:\n",
    "            relative_link = tag.get(\"href\")\n",
    "            if relative_link:\n",
    "                all_links.append({\"url\": \"https://www.truecar.com\" + relative_link})\n",
    "\n",
    "    json_path = f\"./data/truecar_links_{location_key}.json\"\n",
    "    csv_path = f\"./data/truecar_links_{location_key}.csv\"\n",
    "    output_json = f\"./data/truecar_details_{location_key}.json\"\n",
    "    output_csv = f\"./data/truecar_details_{location_key}.csv\"\n",
    "\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(all_links, f, indent=2)\n",
    "    pd.DataFrame(all_links).to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"✅ Saved {len(all_links)} links for {city.title()}, {state.upper()}\")\n",
    "    scrape_all_car_details(json_path=json_path, output_json=output_json, output_csv=output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495076e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "boston_ma - Page: 100%|██████████| 200/200 [06:00<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 6459 links for Boston, MA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping car details: 100%|██████████| 6459/6459 [2:10:36<00:00,  1.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved car details to:\n",
      "- ./data/truecar_details_boston_ma.json\n",
      "- ./data/truecar_details_boston_ma.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "austin_tx - Page: 100%|██████████| 200/200 [05:59<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 6459 links for Austin, TX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping car details: 100%|██████████| 6459/6459 [2:09:25<00:00,  1.20s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved car details to:\n",
      "- ./data/truecar_details_austin_tx.json\n",
      "- ./data/truecar_details_austin_tx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "san-francisco_ca - Page: 100%|██████████| 200/200 [05:42<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 6459 links for San Francisco, CA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping car details:  36%|███▋      | 2353/6459 [48:02<1:24:09,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cities = [(\"Boston\", \"MA\"), (\"Austin\", \"TX\"), (\"San Francisco\", \"CA\")]\n",
    "    for city, state in cities:\n",
    "        scrape_city_state(city, state, max_pages=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
