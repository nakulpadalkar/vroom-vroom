{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class CarsScraper:\n",
    "\n",
    "    def __init__(self, base_url, categories, max_pages=1):\n",
    "        # Initialize the CarsScraper object with the provided base URL, categories, and maximum pages\n",
    "        self.base_url = base_url\n",
    "        self.categories = categories\n",
    "        self.max_pages = max_pages if max_pages else {}\n",
    "        \n",
    "        # Initialize total_car_counts dictionary with each category and its count set to 0\n",
    "        self.total_car_counts = {category: 0 for category in categories}\n",
    "        \n",
    "        # Initialize execution time variables for serial and multithreaded scraping\n",
    "        self.serial_execution_time = 0\n",
    "        self.multithreaded_execution_time = 0\n",
    "\n",
    "    def send_request(self, url):\n",
    "        try:\n",
    "            # Send a GET request to the provided URL using the requests library\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            # Raise an exception for HTTP errors (status codes other than 2xx)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Return the response object\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle any request exceptions and print an error message\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_page(self, response):\n",
    "        # Check if the response is valid and has a status code of 200 (OK)\n",
    "        if response and response.status_code == 200:\n",
    "            # Parse the HTML content of the response using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Return the BeautifulSoup object representing the parsed HTML\n",
    "            return soup\n",
    "        else:\n",
    "            # Handle cases where the response is invalid or has a non-OK status code\n",
    "            print(f\"Error: Invalid response or status code {response}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_car_info(self, car_element, category):\n",
    "        # Initialize an empty dictionary to store car information\n",
    "        car_info = {}\n",
    "\n",
    "        try:\n",
    "            # Extract the model year from the 'h2' element with class 'title'\n",
    "            car_info['model_year'] = car_element.find('h2', class_='title').text.strip()\n",
    "\n",
    "            # Extract the mileage information from the 'div' element with class 'mileage'\n",
    "            mileage_element = car_element.find('div', class_='mileage')\n",
    "            car_info['mileage'] = mileage_element.text.strip() if mileage_element else None\n",
    "\n",
    "            # Extract the price information from the 'span' element with class 'primary-price'\n",
    "            price_element = car_element.find('span', class_='primary-price')\n",
    "            car_info['price'] = price_element.text.strip() if price_element else None\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Handle any attribute errors during extraction and print an error message\n",
    "            print(f\"Error extracting car information: {e}\")\n",
    "\n",
    "        # Add the 'category' key to the car_info dictionary\n",
    "        car_info['category'] = category\n",
    "        \n",
    "        # Return the dictionary containing extracted car information\n",
    "        return car_info\n",
    "\n",
    "\n",
    "    def scrape_category(self, category, max_pages=None):\n",
    "        # Set the maximum number of pages to scrape for the given category\n",
    "        max_pages = max_pages or self.max_pages.get(category, 1)\n",
    "        \n",
    "        # Initialize an empty list to store car information\n",
    "        car_info_list = []\n",
    "\n",
    "        # Iterate through each page up to the specified maximum\n",
    "        for page in range(1, max_pages + 1):\n",
    "            # Construct the category-specific URL for the current page\n",
    "            category_url = f\"{self.base_url}&makes[]={category}&maximum_distance=all&page={page}&stock_type=all&zip=\"\n",
    "            \n",
    "            # Send a request to the constructed URL and get the response\n",
    "            response = self.send_request(category_url)\n",
    "            \n",
    "            # Parse the HTML content of the response using BeautifulSoup\n",
    "            soup = self.parse_page(response)\n",
    "\n",
    "            # Check if the soup is valid\n",
    "            if soup:\n",
    "                # Extract car elements from the parsed HTML\n",
    "                car_elements = soup.find_all('div', class_='vehicle-details')\n",
    "\n",
    "                # Iterate through each car element and extract car information\n",
    "                for car_element in car_elements:\n",
    "                    car_info = self.extract_car_info(car_element, category)\n",
    "                    car_info_list.append(car_info)\n",
    "\n",
    "        # Update the total_car_counts dictionary with the total count for the given category\n",
    "        self.total_car_counts[category] += len(car_info_list) // 2\n",
    "        \n",
    "        # Return the list of car information for the given category\n",
    "        return car_info_list\n",
    "\n",
    "\n",
    "    def save_metadata(self, car_info_list, category_folder):\n",
    "        # Convert the list of car information into a Pandas DataFrame\n",
    "        df = pd.DataFrame(car_info_list)\n",
    "\n",
    "        # Define the path for the CSV file within the category folder\n",
    "        csv_path = os.path.join(category_folder, 'car_info.csv')\n",
    "\n",
    "        # Save the DataFrame as a CSV file without including the index column\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "    def scrape_cars_serial(self):\n",
    "        # Record the start time of the serial execution\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over each category and scrape car information sequentially\n",
    "        for category in self.categories:\n",
    "            # Scrape car information for the current category\n",
    "            car_info_list = self.scrape_category(category)\n",
    "\n",
    "            # Create a folder for each category in the 'output' directory\n",
    "            category_folder = os.path.join('serial_output', category)\n",
    "            os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "            # Save the metadata (car information) for the category in its respective folder\n",
    "            self.save_metadata(car_info_list, category_folder)\n",
    "\n",
    "        # Record the end time of the serial execution\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate and print the serial execution time\n",
    "        self.serial_execution_time = end_time - start_time\n",
    "        print(f\"Serial Execution Time: {self.serial_execution_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    def scrape_cars_multithreaded(self):\n",
    "        # Record the start time of the multithreaded execution\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a ThreadPoolExecutor for concurrent multithreaded execution\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Submit scraping tasks for each category using ThreadPoolExecutor\n",
    "            futures = [executor.submit(self.scrape_category, category) for category in self.categories]\n",
    "\n",
    "            # Iterate over completed futures as they finish\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                # Retrieve the result of each completed future (car information list for a category)\n",
    "                car_info_list = future.result()\n",
    "\n",
    "                # Extract the category from the result\n",
    "                category = car_info_list[0]['category']\n",
    "\n",
    "                # Create a folder for each category in the 'output' directory\n",
    "                category_folder = os.path.join('multithreaded_output', category)\n",
    "                os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "                # Save the metadata (car information) for the category in its respective folder\n",
    "                self.save_metadata(car_info_list, category_folder)\n",
    "\n",
    "        # Record the end time of the multithreaded execution\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate and print the multithreaded execution time\n",
    "        self.multithreaded_execution_time = end_time - start_time\n",
    "        print(f\"Multithreaded Execution Time: {self.multithreaded_execution_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    def summary_chart(self, save_path='summary_chart.png'):\n",
    "        # Plotting the combined chart\n",
    "        categories = list(self.total_car_counts.keys())\n",
    "        car_numbers = list(self.total_car_counts.values())\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Bar chart for car numbers with different colors for each category\n",
    "        category_colors = {category: \"#\" + ''.join([random.choice('0123456789ABCDEF') for _ in range(6)]) for category in categories}\n",
    "        colors = [category_colors[category] for category in categories]\n",
    "\n",
    "        ax1.bar(categories, car_numbers, color=colors, alpha=0.7, label='Number of Cars')\n",
    "        ax1.set_ylabel('Number of Cars', fontsize=12, color='black')\n",
    "        ax1.tick_params('y', labelsize=10, colors='black')\n",
    "\n",
    "        # Line chart for execution time comparison on the same plot\n",
    "        ax2 = ax1.twinx()\n",
    "        execution_times = [self.serial_execution_time, self.multithreaded_execution_time]\n",
    "        ax2.plot(['Serial', 'Multithreaded'], execution_times, marker='o', color='red', label='Execution Time')\n",
    "        ax2.set_ylabel('Execution Time (seconds)', fontsize=12, color='red')\n",
    "\n",
    "        # Beautify the chart\n",
    "        plt.title('Number of Cars Downloaded and Execution Time Comparison', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "\n",
    "        # Save the chart\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"Summary chart saved to {save_path}\")\n",
    "\n",
    "        # Display the chart\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "# Example usage\n",
    "# more categories >> 'acura','audi','cadillac','chevrolet','dodge','ford','honda','hyundai','infiniti','jaguar','bugatti','aston_martin','lexus'\n",
    "        \n",
    "categories = ['volvo', 'bmw', 'jeep', 'cadillac','acura','hyundai','ford','chevrolet']\n",
    "url = \"https://www.cars.com/shopping/results/?makes[]=volvo&maximum_distance=all&stock_type=all&zip=\"\n",
    "\n",
    "# Example with max_pages\n",
    "max_pages = {'volvo': 4, 'bmw': 8, 'jeep': 3, 'cadillac': 5,'acura':2,'hyundai':4,'ford':3,'chevrolet':8}\n",
    "\n",
    "scraper = CarsScraper(url, categories, max_pages)\n",
    "\n",
    "# Phase 1: Serial Implementation\n",
    "scraper.scrape_cars_serial()\n",
    "\n",
    "# Phase 2: Multithreaded Implementation\n",
    "scraper.scrape_cars_multithreaded()\n",
    "\n",
    "# Summary Chart\n",
    "scraper.summary_chart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
