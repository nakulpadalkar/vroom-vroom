{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3869c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CarsScraper:\n",
    "    def __init__(self, base_url, categories, max_pages, zip_code='02140'):\n",
    "        self.base_url = base_url\n",
    "        self.categories = categories\n",
    "        self.max_pages = max_pages\n",
    "        self.zip_code = zip_code\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    def send_request(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"[200 OK] {url}\")\n",
    "            else:\n",
    "                print(f\"[{response.status_code}] Failed to fetch {url}\")\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] {url} -> {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_page(self, response):\n",
    "        if response and response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup\n",
    "        return None\n",
    "\n",
    "    def extract_car_info(self, car_element, category):\n",
    "        car_info = {'category': category}\n",
    "        try:\n",
    "            car_info['model_year'] = car_element.find('h2', class_='title').text.strip()\n",
    "            mileage_elem = car_element.find('div', class_='mileage')\n",
    "            car_info['mileage'] = mileage_elem.text.strip() if mileage_elem else None\n",
    "            price_elem = car_element.find('span', class_='primary-price')\n",
    "            car_info['price'] = price_elem.text.strip() if price_elem else None\n",
    "        except AttributeError as e:\n",
    "            print(f\"Extraction error: {e}\")\n",
    "        return car_info\n",
    "\n",
    "    def scrape_category(self, category, max_pages=None):\n",
    "        max_pages = max_pages or self.max_pages.get(category, 1)\n",
    "        car_info_list = []\n",
    "\n",
    "        print(f\"\\n[SCRAPING] Category: {category} | Pages: {max_pages}\")\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = (\n",
    "                f\"https://www.cars.com/shopping/results/\"\n",
    "                f\"?dealer_id=&include_shippable=true&keyword=\"\n",
    "                f\"&list_price_max=&list_price_min=\"\n",
    "                f\"&makes[]={category}\"\n",
    "                f\"&maximum_distance=10&mileage_max=&monthly_payment=\"\n",
    "                f\"&page={page}&page_size=100\"\n",
    "                f\"&sort=best_match_desc\"\n",
    "                f\"&stock_type=used\"\n",
    "                f\"&year_max=&year_min=\"\n",
    "                f\"&zip={self.zip_code}\"\n",
    "            )\n",
    "\n",
    "            response = self.send_request(url)\n",
    "            soup = self.parse_page(response)\n",
    "\n",
    "            if soup:\n",
    "                car_elements = soup.find_all('div', class_='vehicle-details')\n",
    "                \n",
    "                # Progress bar for number of car elements found\n",
    "                for car_element in tqdm(car_elements, desc=f\"{category} (cars)\", leave=True):\n",
    "                    car_info = self.extract_car_info(car_element, category)\n",
    "                    car_info_list.append(car_info)\n",
    "\n",
    "        return car_info_list\n",
    "\n",
    "\n",
    "    def save_data(self, data, output_dir, category):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df = pd.DataFrame(data)\n",
    "        csv_path = os.path.join(output_dir, f\"{category}_used_cars.csv\")\n",
    "        json_path = os.path.join(output_dir, f\"{category}_used_cars.json\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        df.to_json(json_path, orient=\"records\", indent=2)\n",
    "        print(f\"Saved CSV and JSON for {category}\")\n",
    "\n",
    "    def scrape_all(self, output_dir=\"output\"):\n",
    "        for category in tqdm(self.categories, desc=\"All Categories\", position=0):\n",
    "            data = self.scrape_category(category)\n",
    "            self.save_data(data, output_dir, category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0865edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All Categories:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SCRAPING] Category: bmw | Pages: 1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cars.com/shopping/results/?dealer_id=&include_shippable=true&keyword=&list_price_max=&list_price_min=&makes[]=&maximum_distance=10&mileage_max=&monthly_payment=&page=1&page_size=100&sort=best_match_desc&stock_type=used&year_max=&year_min=&zip=02140\"\n",
    "\n",
    "categories = ['bmw']\n",
    "max_pages = {'bmw': 1}\n",
    "scraper = CarsScraper(base_url=\"\", categories=categories, max_pages=max_pages, zip_code='02140')\n",
    "scraper.scrape_all(output_dir=\"used_car_data_02140\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
